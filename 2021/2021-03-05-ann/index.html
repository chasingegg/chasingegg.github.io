<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta name=author content="Chao.G"><meta name=description content="高超的个人博客"><link rel=prev href=https://chasingegg.github.io/2021/2021-02-10-blog/><link rel=canonical href=https://chasingegg.github.io/2021/2021-03-05-ann/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"><title>关于ANN的一点思考 | Chao.G</title><meta name=title content="关于ANN的一点思考 | Chao.G"><link rel=stylesheet href=/css/main.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/chasingegg.github.io"},"articleSection":"posts","name":"关于ANN的一点思考","headline":"关于ANN的一点思考","description":"这里的ANN，是Approximate Nearest Neighbor的意思，从数据中找到和查询数据最接近的结果，实际上这个问题往往都被扩展成Approx","inLanguage":"zh-cn","author":"Chao.G","creator":"Chao.G","publisher":"Chao.G","accountablePerson":"Chao.G","copyrightHolder":"Chao.G","copyrightYear":"2021","datePublished":"2021-03-05 00:00:00 \u002b0000 UTC","dateModified":"2021-03-05 00:00:00 \u002b0000 UTC","url":"https:\/\/chasingegg.github.io\/2021\/2021-03-05-ann\/","wordCount":"2856","keywords":["ANN","Chao.G"]}</script></head><body><div class=wrapper><nav class=navbar><progress class=content_progress max=0 value=0></progress><div class=container><div class="navbar-header header-back2home-logo"><span class=logo_mark>>$</span>
<a href=https://chasingegg.github.io><span class=logo_text>cd /home/</span>
<span class=logo_cursor></span></a></div><div class=navbar-right><span class=menu><a class=menu-item href=/posts/ title>Blog</a>
<a class=menu-item href=/categories/ title>Categories</a>
<a class=menu-item href=/tags/ title>Tags</a>
<a class=menu-item href=/about/ title>About</a>
<span class=divide></span><a href=javascript:void(0); class=theme-switch><i class="iconfont icon-dark-mode"></i></a></span></div></div></nav><nav class=navbar-mobile id=nav-mobile style=display:none><progress class=content_progress max=0 value=0></progress><div class=container><div class=navbar><div class="navbar-header header-logo"><a href=https://chasingegg.github.io>Chao.G</a></div><div class=navbar-right><div><a href=javascript:void(0); class=theme-switch><i class="iconfont icon-dark-mode"></i></a></div><div class=menu-toggle><span></span><span></span><span></span></div></div></div><div class=menu id=mobile-menu><nav class=mb-md><a class=menu-item href=/posts/ title><h3>Blog</h3><div class=menu-active></div></a><a class=menu-item href=/categories/ title><h3>Categories</h3><div class=menu-active></div></a><a class=menu-item href=/tags/ title><h3>Tags</h3><div class=menu-active></div></a><a class=menu-item href=/about/ title><h3>About</h3><div class=menu-active></div></a></nav></div></div></nav><main class=main><div class=container><article class=post-warp itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline">关于ANN的一点思考</h1><div class=post-meta>Written by <a itemprop=name href=https://chasingegg.github.io rel=author>Chao.G</a> with ♥
<span class=post-time>on <time datetime=2021-03-05 itemprop=datePublished>March 5, 2021</time></span>
in
<i class="iconfont icon-folder"></i><span class=post-category><a href=https://chasingegg.github.io/categories/notes/>Notes,</a></span>
<span class=post-word-count>2856 words</span></div></header><div class=post-content><p>这里的ANN，是Approximate Nearest Neighbor的意思，从数据中找到和查询数据最接近的结果，实际上这个问题往往都被扩展成Approximate K Nearest Neighbors问题，在之后的介绍里ANN指代都是这一种更为一般的情况。</p><p>在过去的一年左右时间里，我都在做ANN上的一些研究，算是经历了一个小白入门科研的完整阶段，也算有了一点体会。本文中的ANN查询都是针对高维向量进行的，所以有时也常常被称为向量检索。这里不对算法进行很具体的分析，希望从一个较为宏观的角度来看待这个问题。</p><h2 id=背景>背景</h2><p>ANN查询问题是一个十分基础的问题，有着很丰富的应用，比如在图片数据库中找到跟我的照片最相似的照片，每个图片通过特征提取模型以后就是一个个的高维向量（一般在100维到1000维之间），相似照片的检索其实质就是ANN查询。使用数学语言表达如下：</p><p>令<span class=math>\(\mathcal{D}=\{x_1, x_2, \cdots, x_n\}\)</span>为数据集，<span class=math>\(x_i \in R^d, dist(x_i, x_j)\)</span>为距离函数(一般为欧式距离)，距离越小，则代表相似度越高。给定查询向量<span class=math>\(q\)</span>, <span class=math>\(k\)</span>近邻<span class=math>\(TopK_q\)</span>都满足，<span class=math>\(\forall v \in TopK_q, \forall x \in \mathcal{D} \setminus TopK_q, dist(v, q) \leq dist(x, q).\)</span></p><p>而实际返回的结果可能有误差，令返回结果为<span class=math>\(TopK_q'\)</span>，使用 <span class=math>\( Recall \)</span> 来衡量准确度，即<span class=math>\(Recall = \frac{| TopK'_q \cap TopK_q |}{k} \)</span>.</p><p>所以实际上衡量一个算法性能的时候，我们是通过参数改变查询的范围，使用<span class=math>\(Recall-Latency\)</span>曲线来直观地展示不同<span class=math>\(Recall\)</span>下的查询性能。</p><h2 id=索引>索引</h2><p>ANN是一个古老的问题，经过了几十年的发展，主要有树，哈希，量化编码，图这几类具有代表性的索引算法。这里要非常感谢<a href=https://yongyuan.name/>袁勇</a>同学，详细归纳了ANN的发展，同时还维护了信息检索的一些最新论文。在我入门这个领域的时候，这几乎是我看过的中文博客里面最为用心，理解也非常深刻的高质量文章了，强烈推荐刚入门的小伙伴去阅读他的文章。</p><p>对于ANN来说，我们希望查询使候选集合较小，过滤掉大部分向量，另外在计算向量距离时希望可以对向量进行降维，编码等操作使得距离计算的时间（对两个几百维的向量进行欧式距离计算是一波很大的开销）可以大幅降低，并减轻存储压力。这两者也常常会结合起来。</p><p>首先看第一点，如何过滤掉无用的向量。回忆我们在学习二叉树时候的情景，我们通过比较和根节点的大小关系，来确定我们接下来要搜索左子树还是右子树不断深入，因为我们要的是近邻，所以有可能会回溯。所以基于树的索引基本沿用了这样一个思路，关键在于如何将数据分裂，可以选择方差最大的维度来分，这是KD-Tree的思路，也可以使用Kmeans，这是Kmeans-Tree，也可以使用哈希来分桶。</p><p>再看第二点，我们如何加快向量距离计算呢？我们可以来看量化编码的思路，尤其是<strong>乘积量化</strong>，我们可以将距离计算转化为查表操作。将向量等分成若干段，每一段使用聚类算法分成若干聚类中心，那么所有向量都可以视作是这一些聚类中心的组合。所以我们现在只需要存储少量的聚类中心，并且可以预计算好聚类中心之间的距离。</p><p>最后单独来说一说图索引，2016年的论文HNSW已经成为了现在使用相当广泛的索引算法，在数据可以全部放进内存的前提下，图索引几乎都是最快的选择。但事实上，每个刚入门的同学都会有些疑惑，平时提到索引几乎都是树型索引，图这个结构是如何作为索引来使用呢？在我看来，图具备很好的连通性，不像树结构，只有一个方向，否则就要进行回溯，而图的话通路就四通八达。一般我们在一维条件下很容易对数据按照某个数值分成大小两堆，这是一种很完美的分割。<strong>而在高维的相似度查询场景下，事实上你很难完美对数据分割，做到一个通路无须回溯的</strong>，而图就可以绕过这个限制，达到树索引几倍甚至几十倍的性能。</p><p><a href=http://ann-benchmarks.com/>ANN-Benchmarks</a>里面列举了不同算法的单机查询性能，有兴趣的读者可以参考。</p><h2 id=机器学习>机器学习</h2><p>使用机器学习的方法代替传统的查询算法或者是数据结构，已经成为了很重要的研究话题。当然这在实际生产环境中能否使用就是另一个问题了，大都会有这样一些问题：无法实时更新，训练时间长等。但无法否认的是，AI确实是未来。</p><p>反映到ANN这个问题上，怎么把机器学习的思路用到这上面，有很多思路。一个直接的思路是将查询向量作为输入，它的近邻作为输出，然后拿一些向量用来训练，对实际的查询再做模型推导。还有的方法是构建图索引的时候构建cost model来选择边，在查询的时候使用模型选择最佳路径等。</p><p>其实我对机器学习是比较不感冒的，有些模型的建立我觉得需要很强的AI背景，而且获得的收益很小，overhead（模型大小，模型训练时间）倒是很大。但是有一篇论文让我有了一个新的认识，是SIGMOD2020的一篇文章，<a href=https://dl.acm.org/doi/pdf/10.1145/3318464.3380600>Improving Approximate Nearest Neighbor Search through Learned Adaptive Early Termination</a>，这篇文章其实讲了一个很简单的故事，作者发现索引上查询的时候，不同查询向量所需的“step”是不同的，对于那些“简单”的查询向量，短短几步就可以结束查询，对于某些“困难”的查询向量，则需要在索引上查询很多的步数才能找到真正的近邻。那么机器学习就可以来学习每个向量所需要的步数。这其实就转化为了一个非常基础的机器学习问题，这种简单性为我们真正使用机器学习方法其实降低了不少门槛。</p><p>这篇文章也直接影响了我们做的ANN研究，从茫然中逐渐提炼出了属于我们的一些想法，在我看来，我们的思路也是非常简单，且具有美感的。等着paper中了再好好来讲讲我们的工作，希望不要等太久。。</p><h2 id=系统>系统</h2><p>支持ANN查询在大数据时代逐渐演变为一个系统所需要支持的需求（也可能是伪需求？）。这一部分阿里巴巴做的很不错，也可能是阿里本身打广告的能力非常之强（误），阿里云数据库团队和蚂蚁的团队分别在自研数据库AnalyticDB和开源数据库Postgres上集成了向量索引，系统名字分别是ADBV和PASE，也分别在VLDB2020和SIGMOD2020的Industrial Track上都发表了论文。有家明星创业公司Zilliz开源了他们的向量检索产品Milvus，当然在我看来这个业务是否太过单一了，但确实人家拿到了相当多的融资，吹水能力也是相当不俗，当然这作为一家初出茅庐的公司是非常重要的。很偶然的机会也接触过这家公司的HR，差点就加入了，不过我应该还会持续关注这家公司的。</p><p>对于系统来说，算法的创新不是重点，重点是生产上的落地。如何做好数据的增删改查，如何管理实时数据和历史数据，如何将索引改造成适合数据库的内存管理等。上述几个系统也都提到了ANN查询和普通结构化查询相结合的融合查询，ADBV较为系统地讨论了融合查询的优化思路，把它做到了数据库的优化器里面。</p><h2 id=总结>总结</h2><p>像ANN这样的“古老”话题，在初期一定是要看很多的论文来加深理解，而且不同的方法又非常多，在调研上就已经需要花上非常长的时间，需要比较归纳算法的特点，并需要一些实验结果的佐证。当然想在上面做出点东西确实是很难很难的。这时候尝试用机器学习或者系统层面来看待问题，也许会有新的发现。</p></div><div class=post-copyright><p class=copyright-item><span>Author:</span>
<span>Chao.G</span></p><p class="copyright-item lincese">本文采用<a rel=license href=http://creativecommons.org/licenses/by-nc/4.0/ target=_blank>知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可</p></div><div class=post-tags><section><i class="iconfont icon-tag"></i>Tag(s):
<span class=tag><a href=https://chasingegg.github.io/tags/ann/>#ANN</a></span></section><section><a href=javascript:window.history.back();>back</a></span> ·
<span><a href=https://chasingegg.github.io>home</a></span></section></div><div class=post-nav><a href=https://chasingegg.github.io/2021/2021-02-10-blog/ class=prev rel=prev title=重开一个blog><i class="iconfont icon-left"></i>&nbsp;重开一个blog</a></div><div class=post-comment><div id=utteranc-container><script src=https://utteranc.es/client.js repo=chasingegg/chasingegg.github.io issue-term=title theme=preferred-color-scheme crossorigin=anonymous async></script></div></div></article></div></main><footer class=footer><div class=copyright>&copy;
<span itemprop=copyrightYear>2021 - 2021</span>
<span class=with-love><i class="iconfont icon-love"></i></span><span class=author itemprop=copyrightHolder><a href=https://chasingegg.github.io>Chao.G</a> |</span>
<span>Powered by <a href=https://gohugo.io/ target=_blank rel="external nofollow">Hugo</a> & <a href=https://github.com/Mogeko/Mogege target=_blank rel="external nofollow">Mogege</a></span></div></footer><script defer src=/js/vendor_main.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css integrity="sha256-V8SV2MO1FUb63Bwht5Wx9x6PVHNa02gv8BgH/uH3ung=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js integrity="sha256-90d2pnfw0r4K8CZAWPko4rpFXQsZvJhTBGYNkipDprI=" crossorigin=anonymous onload=renderMathInElement(document.body)></script><script src=https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin=anonymous></script><script>pangu.spacingPage()</script></div></body></html>