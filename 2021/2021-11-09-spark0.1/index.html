<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta name=author content="Chao.G"><meta name=description content="高超的个人博客"><link rel=prev href=https://chasingegg.github.io/2021/2021-04-10-milvus/><link rel=next href=https://chasingegg.github.io/2022/2022-02-01-farewell-2021/><link rel=canonical href=https://chasingegg.github.io/2021/2021-11-09-spark0.1/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"><title>Spark Alpha-0.1 | Chao.G</title><meta name=title content="Spark Alpha-0.1 | Chao.G"><link rel=stylesheet href=/css/main.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/chasingegg.github.io"},"articleSection":"posts","name":"Spark Alpha-0.1","headline":"Spark Alpha-0.1","description":"Spark无疑是非常广泛使用的大数据引擎了，前一段时间同事去挖了Spark最早release出来的版本给我们做了下介绍，突然觉得很有趣，确实","inLanguage":"zh-cn","author":"Chao.G","creator":"Chao.G","publisher":"Chao.G","accountablePerson":"Chao.G","copyrightHolder":"Chao.G","copyrightYear":"2021","datePublished":"2021-11-09 00:00:00 \u002b0000 UTC","dateModified":"2021-11-09 00:00:00 \u002b0000 UTC","url":"https:\/\/chasingegg.github.io\/2021\/2021-11-09-spark0.1\/","wordCount":"2080","keywords":["spark","Chao.G"]}</script></head><body><div class=wrapper><nav class=navbar><progress class=content_progress max=0 value=0></progress><div class=container><div class="navbar-header header-back2home-logo"><span class=logo_mark>>$</span>
<a href=https://chasingegg.github.io><span class=logo_text>cd /home/</span>
<span class=logo_cursor></span></a></div><div class=navbar-right><span class=menu><a class=menu-item href=/posts/ title>Blog</a>
<a class=menu-item href=/categories/ title>Categories</a>
<a class=menu-item href=/tags/ title>Tags</a>
<a class=menu-item href=/about/ title>About</a>
<span class=divide></span><a href=javascript:void(0); class=theme-switch><i class="iconfont icon-dark-mode"></i></a></span></div></div></nav><nav class=navbar-mobile id=nav-mobile style=display:none><progress class=content_progress max=0 value=0></progress><div class=container><div class=navbar><div class="navbar-header header-logo"><a href=https://chasingegg.github.io>Chao.G</a></div><div class=navbar-right><div><a href=javascript:void(0); class=theme-switch><i class="iconfont icon-dark-mode"></i></a></div><div class=menu-toggle><span></span><span></span><span></span></div></div></div><div class=menu id=mobile-menu><nav class=mb-md><a class=menu-item href=/posts/ title><h3>Blog</h3><div class=menu-active></div></a><a class=menu-item href=/categories/ title><h3>Categories</h3><div class=menu-active></div></a><a class=menu-item href=/tags/ title><h3>Tags</h3><div class=menu-active></div></a><a class=menu-item href=/about/ title><h3>About</h3><div class=menu-active></div></a></nav></div></div></nav><main class=main><div class=container><article class=post-warp itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline">Spark Alpha-0.1</h1><div class=post-meta>Written by <a itemprop=name href=https://chasingegg.github.io rel=author>Chao.G</a> with ♥
<span class=post-time>on <time datetime=2021-11-09 itemprop=datePublished>November 9, 2021</time></span>
in
<i class="iconfont icon-folder"></i><span class=post-category><a href=https://chasingegg.github.io/categories/notes/>Notes,</a></span>
<span class=post-word-count>2080 words</span></div></header><div class=post-content><p>Spark无疑是非常广泛使用的大数据引擎了，前一段时间同事去挖了Spark最早release出来的版本给我们做了下介绍，突然觉得很有趣，确实代码一直都是有记忆的，最早版本的Spark其实一直在github上静静地躺着，无数后面的故事都是从这个初版的代码发展而来。</p><p>想学习Spark的基本概念其实从这个0.1版本的代码，搭配原始论文<a href=https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf>Spark: Cluster Computing with Working Sets</a>一起来看，会发现一些核心的思路在这时候就已经基本都有了，同时又可以让我们很容易去理解Spark的一些设计思路。整个论文给我的感觉是很"实在"，介绍了设计和实现的细节，有一种大作业写一个toy system的既视感，作者应该也没想到Spark会发展成现在的模样。</p><h2 id=motivation>Motivation</h2><p>Spark想解决一个问题，就是数据（在内存中）的复用性，作者发现在这种情况下MapReduce不是很高效</p><ul><li>迭代式任务：机器学习算法往往需要重复作用一个数据集去优化一个参数，每一次迭代都可以表示成MapReduce任务，都需要重新从磁盘加载数据。</li><li>迭代式分析：用户很自然地需要加载数据集到内存中，多次对数据集做查询分析，但对于MapReduce任务而言，每次查询都是一次单独的MapReduce任务，需要重新从磁盘加载数据。</li></ul><p>所以可以看到，其实上面两个问题的本质问题都是一个，而且看起来似乎很trivial，甚至有点疑惑为啥MapReduce不做这个优化，我的理解是MapReduce的主要目标还是在于解决scalability和fault tolerance两个问题，当时的系统都是disk-based的，而且事实证明，Spark想做的内存计算也并不是一件容易的事情。</p><h2 id=编程模型>编程模型</h2><p>Spark提供了两种抽象，RDD（Resilient Distributed Dataset）和Parallel Operations（可以认为是作用在RDD上的计算）。此外Spark还提供两种特别的共享变量：广播变量和累加器。</p><h3 id=rdd>RDD</h3><p>RDD是一种只读的分布式数据对象，可以有四种构建RDD的方式</p><ul><li>From a file，从文件系统里读取。</li><li>Parallelizing a scala collection，比如把scala的数组转化成几个分片分散到多台机器上。</li><li>Transforming an exsiting RDD，通过一些map操作做RDD转化。</li><li>Changing the persistence of an exsiting RDD 默认状态下RDD是lazy的，需要的时候会计算出来，然后在内存中被移除，但用户可以定义cache让它在内存中保存或者save持久化到文件系统上。</li></ul><p>每个RDD都会记录自己是如何创建的，也就是血缘，帮助计算失败的场景下重新构建RDD。</p><p>每个RDD会实现下面几个方法</p><ul><li>getPartitions，获取当前RDD的分区id列表。</li><li>getIterator(partition)，遍历该分区下的数据。</li><li>getPreferredLocations(partition)，用于任务调度实现数据局部性。</li></ul><p>任务提交以后，每个任务会被发送到自己prefer的某个分区上，然后去遍历该分区的数据。</p><h3 id=parallel-operations>Parallel Operations</h3><p>作用在RDD上的一些计算</p><ul><li>reduce，把数据结果合并，不支持grouped reduce operation，比如group by或者join操作，reduce结果只能在driver上聚合。</li><li>collect，发送数据集中的所有数据到driver。</li><li>foreach，把每一个element经过一个用户定义的函数。</li></ul><h3 id=shared-variables>Shared variables</h3><p>当Spark发送任务到worker节点的时候，本质是传递闭包，这些闭包包含了创建时可见的局部变量，这些变量也会拷贝到对应节点。为了易用性和性能方面需求，Spark提供两种类型的共享变量：</p><ul><li>广播变量，如果一个很大的数据在多个Parallel Operations里用到，最好就是在每个节点上有一份就好，而不是在每一个闭包里都打包进去，这样可以避免一台机器上有一份数据的多份拷贝，节省内存资源。</li><li>累加器，这个变量是在worker和driver上都会有，worker只能按照某个操作往上累加，只有driver可以读，可以方便用来做求和运算。</li></ul><p>用户创建一个广播变量以后，会被保存到分布式文件系统上面，序列化的部分仅仅是文件路径。</p><p>累加器在创建的时候会有一个唯一标识的ID，在每一个任务里面，累加器的一个副本会创建，同时将初始值设成0，最后把累加信息返回给driver，driver接收各分区的任务结果更新最终结果。</p><h2 id=示例>示例</h2><p>来看一个逻辑回归的例子</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=color:#66d9ef>val</span> data <span style=color:#66d9ef>=</span> sc<span style=color:#f92672>.</span>textFile<span style=color:#f92672>(...)</span> <span style=color:#75715e>// read data
</span><span style=color:#75715e></span><span style=color:#66d9ef>var</span> w <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>Vector</span><span style=color:#f92672>.</span>random<span style=color:#f92672>(</span>D<span style=color:#f92672>)</span>  <span style=color:#75715e>// initialize w
</span><span style=color:#75715e></span>
<span style=color:#66d9ef>for</span> <span style=color:#f92672>(</span>i <span style=color:#66d9ef>&lt;-</span> <span style=color:#ae81ff>1</span> to <span style=color:#a6e22e>ITERATIONS</span><span style=color:#f92672>)</span> <span style=color:#f92672>{</span>
  <span style=color:#66d9ef>val</span> gradient <span style=color:#66d9ef>=</span> sc<span style=color:#f92672>.</span>accumulator<span style=color:#f92672>(</span><span style=color:#a6e22e>Vector</span><span style=color:#f92672>.</span>zeros<span style=color:#f92672>(</span>D<span style=color:#f92672>))</span>
  <span style=color:#66d9ef>for</span> <span style=color:#f92672>(</span>p <span style=color:#66d9ef>&lt;-</span> sc<span style=color:#f92672>.</span>parallelize<span style=color:#f92672>(</span>data<span style=color:#f92672>,</span> numSlices<span style=color:#f92672>))</span> <span style=color:#f92672>{</span> <span style=color:#75715e>// run in parallel
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>val</span> scale <span style=color:#66d9ef>=</span> <span style=color:#f92672>(</span><span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> <span style=color:#f92672>(</span><span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> exp<span style=color:#f92672>(-</span>p<span style=color:#f92672>.</span>y <span style=color:#f92672>*</span> <span style=color:#f92672>(</span>w dot p<span style=color:#f92672>.</span>x<span style=color:#f92672>)))</span> <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>)</span> <span style=color:#f92672>*</span> p<span style=color:#f92672>.</span>y
    gradient <span style=color:#f92672>+=</span>  scale <span style=color:#f92672>*</span> p<span style=color:#f92672>.</span>x
  <span style=color:#f92672>}</span>
<span style=color:#f92672>}</span>
w <span style=color:#f92672>-=</span> gradient<span style=color:#f92672>.</span>value</code></pre></div><p>定义gradient为累加器，for循环会启动一个foreach任务，按照数据分片进行并行计算，最后将结果累加到gradient上。</p><h2 id=未来工作>未来工作</h2><p>作者最后规划了下未来的工作，很厉害的是这几点规划恰好是Spark未来重点突破的一些方向，而且都做成功了。</p><ul><li>正式归纳RDD的特点，说明其适合的workloads和应用场景。</li><li>提升RDD的抽象化能力，让开发人员可以自己做存储空间和重新构建RDD之间的权衡。</li><li>支持Shuffle操作，帮助实现像group by和join一类的操作。</li><li>提供更好的交互式能力，比如SQL能力的支持</li></ul><h2 id=reference>Reference</h2><ul><li><a href=https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf>Spark: Cluster Computing with Working Sets</a></li><li><a href=https://github.com/apache/spark/tree/alpha-0.1>Spark-alpha-0.1</a></li></ul></div><div class=post-copyright><p class=copyright-item><span>Author:</span>
<span>Chao.G</span></p><p class="copyright-item lincese">本文采用<a rel=license href=http://creativecommons.org/licenses/by-nc/4.0/ target=_blank>知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可</p></div><div class=post-tags><section><i class="iconfont icon-tag"></i>Tag(s):
<span class=tag><a href=https://chasingegg.github.io/tags/spark/>#spark</a></span></section><section><a href=javascript:window.history.back();>back</a></span> ·
<span><a href=https://chasingegg.github.io>home</a></span></section></div><div class=post-nav><a href=https://chasingegg.github.io/2021/2021-04-10-milvus/ class=prev rel=prev title=Milvus论文解读><i class="iconfont icon-left"></i>&nbsp;Milvus论文解读</a>
<a href=https://chasingegg.github.io/2022/2022-02-01-farewell-2021/ class=next rel=next title=再见2021>再见2021&nbsp;<i class="iconfont icon-right"></i></a></div><div class=post-comment><div id=utteranc-container><script src=https://utteranc.es/client.js repo=chasingegg/chasingegg.github.io issue-term=title theme=preferred-color-scheme crossorigin=anonymous async></script></div></div></article></div></main><footer class=footer><div class=copyright>&copy;
<span itemprop=copyrightYear>2021 - 2022</span>
<span class=with-love><i class="iconfont icon-love"></i></span><span class=author itemprop=copyrightHolder><a href=https://chasingegg.github.io>Chao.G</a> |</span>
<span>Powered by <a href=https://gohugo.io/ target=_blank rel="external nofollow">Hugo</a> & <a href=https://github.com/Mogeko/Mogege target=_blank rel="external nofollow">Mogege</a></span></div></footer><script defer src=/js/vendor_main.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css integrity="sha256-V8SV2MO1FUb63Bwht5Wx9x6PVHNa02gv8BgH/uH3ung=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js integrity="sha256-90d2pnfw0r4K8CZAWPko4rpFXQsZvJhTBGYNkipDprI=" crossorigin=anonymous onload=renderMathInElement(document.body)></script><script src=https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin=anonymous></script><script>pangu.spacingPage()</script></div></body></html>