---
author: "Chao.G"
title: "RabitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search"
date: "2024-10-19"
markup: "mmark"
draft: false
tags: ["ann"]
categories: ["Papers"]
---

今天分享一篇发在SIGMOD2024上跟量化（Quantization）有关的论文，量化在大模型和ANNS等领域都是一个非常重要的技术，主要的思路就是把原数据表示成低精度的形式，通常可以以少量的精度损失换来更少的存储和更快的计算效率。在ANNS里，PQ（Product Quantization）和SQ（Scalar Quantization）是两类非常常用的方法，SQ比较简单，就是把浮点数通过分布的映射关系转成更小的表示比如INT8，PQ稍微复杂一些，具体的过程有兴趣可以去看之前ScaNN的分享。通常来说相同的压缩比例，PQ的精度损失更少。这篇文章主要围绕PQ作为baseline展开。

## Overview

PQ的大概过程是在索引构建阶段，通过聚类的方法构建出codebook，而通过codebook可以拼成原始向量的近似表示，当查询阶段时，query向量和量化后的base向量的距离被当成是真实距离计算。而在这两个阶段都没有任何误差的理论保证，这在某些数据集上就会有很大的误差。

文章提出了一种新的量化算法，RabitQ（Randomly transformed bi-valued vectors for quantizing data vectors）。首先需要把base vector归一化到D维空间的单位球体中，在构建codebook的阶段，有$2^D$个二值化向量，向量每个维度的可选值是$-1/\sqrt{D}$或是$+1/\sqrt{D}$，这个集合是均匀地分布在高维球体中，然后加入了一些随机性，给这个集合乘上一个随机正交矩阵。而每个base向量就会从这个codebook中找到最近的向量作为量化后的表示。而这个codebook其实会有很好的几何性质，为分析理论的误差提供了可能。而在查询阶段计算距离时，不再是简单的query向量和量化后的向量的距离作为真实距离，而需要一个estimator。这个方法不仅提供了误差的理论bound，同时在实际的数据集上也有更小的实际误差。

进一步地，论文给出了高效的计算方法，跟传统的PQ相比，因为使用二值化的编码，所以使用位操作会更加高效。

## RabitQ算法

接下来会逐步来介绍具体算法是如何做的。首先给出论文中的符号表。

![notation](/assets/rabitq-notation.png)

### 原始向量归一化成单位向量

因为向量间的距离通常是unbounded，这对于分析理论误差会很有难度，为了解决这个问题，首先需要把原始向量归一化成单位向量。令$c$为原始向量的centroid，把base向量和query向量归一化的表示成$o = \frac{o_r - c}{||o_r - c||}, q = \frac{q_r - c}{||q_r - c||}$，就可以得到距离的表示（以L2距离为例，COSINE和IP也可以很容易推导）

$$
||o_r - q_r|| = ||(o_r - c) - (q_r - c)||^2 = ||o_r - c||^2 + ||q_r - c||^2 - 2 ||o_c - c|| \cdot ||q_r - c|| \cdot <q, o>
$$

在这个式子中，$||o_r - c||$是建索引阶段可以预计算的，$||q_r - c||$的计算比较轻量，可以忽略，所以问题就转化成了$<q, o>$的计算。这件事的几何解释是，数据针对centroid做归一化就是一种均匀把数据分布在高维球面的操作，减少了数据的skewness。

### Codebook构建

在上一节，我们把原始向量转成均匀分布的单位向量以后，直观上我们应该把codebook也均匀分布在高维球面上，即

$$
C := \{ + \frac{1}{\sqrt{D}}, -\frac{1}{\sqrt{D}} \}^D
$$

而这个codebook的表达空间是$2^D$。但是可能会有一些问题是两个并不相似的向量可能会被量化到同一个向量中，比如两个不相似的向量$(1/\sqrt{D}, ..., 1/\sqrt{D})$和$(1, 0, ..., 0)$，所以需要给codebook加入一些随机性。这个点我觉得直观上理解是这样的codebook过于均匀了，更容易让两个并不相似的点被量化到了同一个codebook的向量。而通过一个矩阵的旋转，可以让codebook中的这些向量随机地偏移到另一个位置。即

$$
C_{rand} := \{ Px | x \in C \}
$$

而只需要存储随机矩阵P，就可以完成codebook的存储，非常简单漂亮。

### 距离estimator

上面提到，距离的计算可以转化成$<o, q>$的计算，那么很自然地就会把问题转化成query向量和量化后的向量的距离，即$<\bar{o}, q>$。但是论文提供了一个更加合适的距离估计算法，这也是这篇论文的核心部分，会有比较多的数学证明，感兴趣的同学可以看原论文。

![geo](/assets/rabitq-geo.png)

当$o$和$q$是平行向量时，可以很容易得到$<\bar{o}, q> = <\bar{o}, o> \cdot <o, q>$，当$o$和$q$非平行向量时，$<\bar{o}, q> = <\bar{o}, o> \cdot <o, q> + <\bar{o}, e_1> \cdot \sqrt{1 - <o, q>^2}, e_1 := \frac{q - <q, o>o}{||q - <q, o>o||}$。光看这个式子并不是很直观，其实就是$q$和$\bar{o}$相对$o$的平行分量的点积和垂直分量的点积之和。论文中主要证明了这个垂直分量的点积的期望是0，所以可以用下式来估计距离，

$$
<o, q> = \frac{<\bar{o}, q>}{<\bar{o}, o>}
$$

而$<\bar{o}, o>$是可以预计算好的，问题就转化成对$<\bar{o}, q>$的计算。而$<\bar{o}, q> = <P\bar{x}, q> = <P^{-1}P\bar{x}, P^{-1}q>= <\bar{x}, q^{'}>$，$\bar{x}$是一个二值化的向量，可选值是$+\frac{1}{\sqrt{D}}/-\frac{1}{\sqrt{D}}$，而存储只需要0/1即可，而$q^{'}$是一个float向量。作者提供了一个高效并且不会过分损失精度的优化，也非常直观，就是把$q^{'}$进一步做SQ转化成$B_q$ bit的int向量，最后实验分析得出4bit是一个比较合理的数值。所以最后问题转化成binary向量和4bit向量的高效距离计算。这个部分其实和之前分享的一篇[文章](https://zhuanlan.zhihu.com/p/690044575)提到的计算模式非常相似，一种是如下图的single mode，可以把4bit向量分解成4个binary向量，这样做4次bitwise的SIMD操作即可。另一种是batch mode，类似ScaNN同时计算一批向量。

![bitwise](/assets/rabitq-bitwise.png)

### RabitQ集成到ANN查询

论文指出了一个future work，就是ScaNN这类batch模式和图索引的结合，因为在图索引上的搜索过程是点对点的计算，无法充分利用上SIMD的优势。而RabitQ和IVF的结合就会比较直观，因为IVF的数据排布是有局部性的，可以点对批的计算，这和ScaNN的逻辑是差不多的，后续的实验也是围绕这个实现来进行的。

还有一个重要的点是re-ranking的步骤，因为量化损失的精度，通常来说搜索出topk * ratio个结果，再拿这个结果去用原始向量做一次精确排序，而这个参数就是需要额外去调的。但是因为RabitQ有一个error bound，就可以利用距离下界剪枝掉不可能在结果中的向量，不需要去人工调整re-ranking的比例。


## 实验

比较了RabitQ对比普通PQ，以及4bitPQ（ScaNN），PQ的优化版本OPQ/LSQ的精度，可以看到在精度上有明显的优势，并且这个优势还是基于RabitQ更加激进的压缩比例得到的。而在计算上几乎和ScaNN的性能优化可以让RabitQ在recall-qps曲线上也有一定优势。

![exp-recall](/assets/rabitq-recall.png)


![exp-perf](/assets/rabitq-perf.png)

## 总结

这是一篇非常偏数学的算法文章，虽然此处大部分证明都被我略过。。但是看上去仍然有不错的实用价值。量化基本上的思路是通过数据的分布来拟合一些代表性的点做到量化的目的，但是这篇文章的思路是给出量化的点，让数据去贴合这些量化点，并且通过一个简单的偏移矩阵就达到了很好的适配效果，这一点非常地impressive。

并且作者仍然做了非常多的工作去让这个work可以真正地应用上去，这一点也不是特别容易，也增加了这个算法真正落地的可能。

当然似乎看上去这个算法只能做float向量->binary向量的量化，也就是1/32的压缩。但是作者已经有了一个工作去做任意比例压缩的量化方法，也就是这篇工作的扩展，期待后面来看看这个文章。