---
author: "Chao.G"
title: "Binary Embedding-based Retrieval at Tencent"
date: "2024-03-31"
draft: false
tags: ["ann"]
categories: ["Papers"]
---

今天介绍一篇腾讯发表在KDD2023的文章，[Binary Embedding-based Retrieval at Tencent](https://arxiv.org/pdf/2302.08714.pdf)。最近binary vector（向量的每一维使用1bit表示）开始有一种火的趋势，cohere的Embed v3模型可以直接支持产生int8和binary vector，还有很多[工作](https://qdrant.tech/articles/binary-quantization-openai)是在向量检索中将float vector通过量化的手段转化成binary vector来做计算。[gaocegege](https://github.com/gaocegege)最近也有一篇[博客](https://blog.pgvecto.rs/my-binary-vector-search-is-better-than-your-fp32-vectors)给出了一些benchmark结果，有非常显著的性能提升。显而易见，binary vector的优势是内存显著降低，计算效率高（SIMD做位运算），但是毕竟向量表达的空间变小了，会带来精度的下降。

## Overview

首先可以看这篇文章产生binary vector是在哪个阶段，如下图所示，是在embedding模型产生向量以后，使用模型来做embedding-to-embedding的转换，可以认为是一种neural binary quantization的手段，和产生向量的embedding模型没有耦合关系。后续的查询可以基于binary vector来做，而且从文章的实验结果来看是不需要原始向量做refine这一过程的，说明几乎没有精度损失？

![bihnsw-overall](/assets/bihnsw-overall.png)

## Recurrent Binarization

文章采用了一种recurrent binarization的方式，可以很好地做到存储空间和精度之间的trade off。对于我们朴素的想法来说，每一维的值从float转成binary，顶多就是输出维度可选，可以从n维的float向量转化成m维的binary向量。但是recurrent binarization的训练过程，会把产生的binary vector再过一个网络重建出float vector和原始向量比较计算出残差，再将残差继续训练出一个binary vector，乘上1/2的weight叠加上去，即$$b_1 = b_0 + r_0$$。看下面这个图就很直观，通过控制网络的输出维度和残差块的个数来共同决定最后产生的向量大小。最后的结果其实是多个binary vector的叠加。

![bihnsw-rb](/assets/bihnsw-recurrent-binarization.png)

计算的时候可以拆解成多个binary vector之间的距离计算，充分利用SIMD做位运算的优势。

![bihnsw-op](/assets/bihnsw-op.png)

## SIMD优化

而本文充分参考了类似ScaNN的计算方式，有兴趣的同学可以去看之前的[分享](https://zhuanlan.zhihu.com/p/684898701)。简单来说，把float vector量化成4bit的表示，然后每一段quantizer的距离采用int8，就可以把Lookup table放在寄存器里，把内存查表操作转换成SIMD操作。不同的是，现在不需要再对向量做量化了，比如以下图为例，使用3个残差块，每一维度本来就是4bit的表示，并且Lookup table在每个quantizer上是完全一致的，并且距离本身就是可以int8的空间完全表示（因为两个4bit的向量的距离可以用256bit表示），所以这种计算方式并没在recurrent binary vector基础上做进一步的精度损失。

![binary-storage](/assets/binary-storage.png)

## 总结

这篇文章可以认为是介绍了一种binary quantization的方式，没有直接对embedding模型产生影响，是一个完全可以在向量数据库可以完成的事情，而且最终输出的结果算是多个binary vector的叠加，可以平衡一些纯binary vector的精度损失，并基于这种特殊形式对计算做了一些优化。

其实对于这一波降低向量的存储空间的浪潮，是降低成本的一个非常好的手段。当然这个量化的动作应该发生在哪个阶段还是值得观望的一个方向，向量数据库拿到向量以后去做这个事情自然会困难一些，但还是目前比较常见的方式。但是cohere的新模型可能已经说明embedding模型也开始朝着这个方向去走了，如果可以直接提供这类压缩后的向量，对于向量索引而言问题会变得简化一些。甚至可以幻想一下以后模型可以产生int2/int4的向量，给到用户更多的选择。

## Reference

- https://arxiv.org/pdf/2302.08714.pdf
- https://arxiv.org/pdf/1802.06466.pdf
- https://openai.com/blog/new-embedding-models-and-api-updates#native-support-for-shortening-embeddings
- https://blog.pgvecto.rs/my-binary-vector-search-is-better-than-your-fp32-vectors
- https://txt.cohere.com/int8-binary-embeddings
- https://qdrant.tech/articles/binary-quantization-openai
- https://zhuanlan.zhihu.com/p/652431207
- https://github.com/TencentARC/BEBR
